{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027b86d1",
   "metadata": {},
   "source": [
    "# üéØ Self-Improving Recommendation System - Quick Start\n",
    "\n",
    "This notebook demonstrates the complete pipeline for training an RL-based recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ba076",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "938a18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32e808",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Download & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fe0273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at ..\\data\\raw\\ml-1m\n",
      "Dataset verification passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.download import download_movielens, verify_dataset\n",
    "\n",
    "# Download MovieLens 1M dataset\n",
    "data_path = download_movielens('movielens-1m', '../data/raw')\n",
    "verify_dataset(data_path, 'movielens-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ff91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.preprocess import preprocess_data\n",
    "\n",
    "# Preprocess the data\n",
    "processed_data = preprocess_data(\n",
    "    data_path='../data/raw/ml-1m',\n",
    "    output_path='../data/processed',\n",
    "    min_user_interactions=20,\n",
    "    min_item_interactions=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59661f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "print(f\"Number of users: {processed_data['n_users']}\")\n",
    "print(f\"Number of items: {processed_data['n_items']}\")\n",
    "print(f\"Training interactions: {len(processed_data['train_df'])}\")\n",
    "print(f\"Validation interactions: {len(processed_data['val_df'])}\")\n",
    "print(f\"Test interactions: {len(processed_data['test_df'])}\")\n",
    "\n",
    "# Show sample data\n",
    "processed_data['train_df'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f113c1",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Train Baseline Model (Matrix Factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.matrix_factorization import MatrixFactorization\n",
    "\n",
    "# Initialize and train baseline\n",
    "embedding_dim = 64\n",
    "\n",
    "baseline = MatrixFactorization(\n",
    "    n_users=processed_data['n_users'],\n",
    "    n_items=processed_data['n_items'],\n",
    "    embedding_dim=embedding_dim,\n",
    "    method='svd'\n",
    ")\n",
    "\n",
    "baseline.fit(processed_data['train_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "user_embeddings = baseline.get_all_user_embeddings()\n",
    "item_embeddings = baseline.get_all_item_embeddings()\n",
    "\n",
    "print(f\"User embeddings shape: {user_embeddings.shape}\")\n",
    "print(f\"Item embeddings shape: {item_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline model\n",
    "import os\n",
    "os.makedirs('../results/models', exist_ok=True)\n",
    "baseline.save('../results/models/baseline.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5a1ad",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Explore User Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e8cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.environment.user_simulator import UserSimulator, FeedbackConfig\n",
    "\n",
    "# Create simulator\n",
    "simulator = UserSimulator(\n",
    "    user_embeddings=user_embeddings,\n",
    "    item_embeddings=item_embeddings,\n",
    "    config=FeedbackConfig(\n",
    "        purchase_threshold=0.7,\n",
    "        click_threshold=0.4,\n",
    "        reward_purchase=5.0,\n",
    "        reward_click_dwell=2.0,\n",
    "        reward_skip=-1.0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a user session\n",
    "user_idx = simulator.reset(user_idx=0)\n",
    "print(f\"Session started for user {user_idx}\")\n",
    "\n",
    "# Simulate 10 recommendations\n",
    "for step in range(10):\n",
    "    # Get random item\n",
    "    item_idx = np.random.randint(0, len(item_embeddings))\n",
    "    \n",
    "    # Get feedback\n",
    "    reward, info = simulator.get_feedback(item_idx)\n",
    "    \n",
    "    print(f\"Step {step+1}: Item {item_idx} -> {info['feedback_type']} (reward: {reward:.1f}, sim: {info['similarity']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea96a1c",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Create Recommendation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2558c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.environment.recommender_env import RecommenderEnv\n",
    "\n",
    "# Create environment\n",
    "env = RecommenderEnv(\n",
    "    user_embeddings=user_embeddings,\n",
    "    item_embeddings=item_embeddings,\n",
    "    max_steps=20,\n",
    "    num_candidates=100,\n",
    "    history_length=10\n",
    ")\n",
    "\n",
    "print(f\"State space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ef4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test environment with random actions\n",
    "state, info = env.reset()\n",
    "print(f\"Initial state shape: {state.shape}\")\n",
    "\n",
    "total_reward = 0\n",
    "done = False\n",
    "step = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    step += 1\n",
    "\n",
    "print(f\"Episode finished after {step} steps\")\n",
    "print(f\"Total reward: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3134c54",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Initialize DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.dqn_agent import DQNAgent\n",
    "\n",
    "# Get state dimension\n",
    "assert env.observation_space.shape is not None, \"Observation space shape must be defined\"\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "\n",
    "# Initialize agent\n",
    "agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_candidates=100,\n",
    "    hidden_layers=[256, 128],\n",
    "    learning_rate=0.001,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    buffer_size=10000,\n",
    "    batch_size=64,\n",
    "    double_dqn=True\n",
    ")\n",
    "\n",
    "print(f\"Agent initialized on device: {agent.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0890d",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Quick Training Demo (100 episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e010b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 100\n",
    "min_replay_size = 500\n",
    "\n",
    "# Track metrics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training\"):\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Get candidate embeddings\n",
    "        candidate_embeddings = env.get_candidate_embeddings()\n",
    "        \n",
    "        # Select action\n",
    "        action = agent.select_action(state, candidate_embeddings, training=True)\n",
    "        \n",
    "        # Take step\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Store experience\n",
    "        agent.store_experience(\n",
    "            state, action, reward, next_state, done, candidate_embeddings\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        if len(agent.replay_buffer) >= min_replay_size:\n",
    "            agent.train()\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(info.get('step', env.max_steps))\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final epsilon: {agent.epsilon:.3f}\")\n",
    "print(f\"Average reward (last 20): {np.mean(episode_rewards[-20:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1606faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Rewards\n",
    "ax1.plot(episode_rewards, alpha=0.6)\n",
    "window = 10\n",
    "smoothed = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(window-1, len(episode_rewards)), smoothed, color='red', linewidth=2)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Cumulative Reward')\n",
    "ax1.set_title('Learning Curve')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode lengths\n",
    "ax2.plot(episode_lengths, alpha=0.6)\n",
    "ax2.axhline(y=env.max_steps, color='green', linestyle='--', label='Max steps')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "ax2.set_title('Session Length Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1822014f",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Evaluate Agent vs Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc5e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, agent=None, num_episodes=50):\n",
    "    \"\"\"Evaluate a policy (agent or random)\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if agent:\n",
    "                candidate_embeddings = env.get_candidate_embeddings()\n",
    "                action = agent.select_action(state, candidate_embeddings, training=False)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            \n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating RL Agent...\")\n",
    "rl_rewards = evaluate_policy(env, agent, num_episodes=50)\n",
    "\n",
    "print(\"Evaluating Random Policy...\")\n",
    "random_rewards = evaluate_policy(env, agent=None, num_episodes=50)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"RL Agent:     {np.mean(rl_rewards):.2f} ¬± {np.std(rl_rewards):.2f}\")\n",
    "print(f\"Random:       {np.mean(random_rewards):.2f} ¬± {np.std(random_rewards):.2f}\")\n",
    "print(f\"Improvement:  {(np.mean(rl_rewards) - np.mean(random_rewards)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34abc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "positions = [1, 2]\n",
    "data = [random_rewards, rl_rewards]\n",
    "bp = ax.boxplot(data, positions=positions, widths=0.6, patch_artist=True)\n",
    "\n",
    "colors = ['#ff6b6b', '#45b7d1']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(['Random Policy', 'DQN Agent'])\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.set_title('Policy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275272d",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Downloaded and preprocessed** MovieLens 1M dataset\n",
    "2. **Trained a baseline** Matrix Factorization model\n",
    "3. **Built a user simulator** based on embedding similarity\n",
    "4. **Created a Gym environment** for RL training\n",
    "5. **Implemented a DQN agent** with experience replay\n",
    "6. **Trained and evaluated** the agent\n",
    "\n",
    "### Key Interview Points:\n",
    "\n",
    "- \"The user simulator uses cosine similarity between user and item embeddings to generate probabilistic feedback\"\n",
    "- \"We use DQN with experience replay to break correlation between samples\"\n",
    "- \"The reward function is designed to optimize long-term engagement, not just immediate clicks\"\n",
    "- \"Œµ-greedy exploration ensures the agent discovers new user preferences\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
